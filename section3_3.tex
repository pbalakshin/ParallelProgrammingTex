{ %section3_3
	\subsection{OpenMP Technology}
	\label{OpenMP:section}
	\par\textbf{Brief description of the technology.} The first version of the OpenMP standard appeared in 1997 with the support of the largest IT companies in the world (Intel, IBM, AMD, HP, Nvidia, etc.). The aim of the new standard was to offer a cross-platform parallelization tool that would be higher-level than the flow control APIs offered by the operating system. OpenMP is currently standardized for three programming languages: C, C ++ and Fortran.
	\par\textbf{Compiler support} The vast majority of existing modern C / C ++ compilers support OpenMP version 2.0 (for example, both gcc and Visual Studio). However, only a few compilers support the newer version of OpenMP 4.0, therefore, when setting forth the material, the OpenMP 2.0 technology will be used as the "common denominator."\
	\par OpenMP defines a set of directives to the preprocessor, which instructs the compiler to replace the source code that follows them with a parallel version using tools available to the compiler, for example, POSIX Threads on Linux or Windows Threads on Microsoft operating systems. For correct translation of directives, you must specify a special key during compilation, the value of which depends on the compiler (examples are given in the Table~\ref{compilerOpenMP:table}).
	\begin{table}[H]
		\caption{Compiler keys to run OpenMP}
		\label{compilerOpenMP:table}
		\begin{center}
			\begin{tabular}{|c|c|}
				\hline
				\textbf{Compiler name} & \specialcell{\textbf{Compiler Key} \\  \textbf{OpenMP}} \\
				\hline
				Gcc & -fopenmp \\
				\hline
				icc (Intel C/C++ compiler) & -openmp \\
				\hline
				Sun C/C++ compiler & -xopenmp \\
				\hline
				Visual Studio C/C++ compiler & /openmp \\
				\hline
				PGI (Nvidia C/C++ compiler) & -mp \\
				\hline
			\end{tabular}
		\end{center}
	\end{table}
	\par In addition to preprocessor directives, OpenMP defines a set of library functions. To call them in the source code, you need to include the OpenMP header file:
	\begin{figure}[H]
		\lstinputlisting{includeOpenMP.cpp}
	\end{figure}
	\par\textbf{Distinctive features.} Among other parallelization technologies, OpenMP stands out for the following important and characteristics:
	\begin{itemize}
		\item Incremental parallelization.
		\item Backward compatibility.
		\item High level of abstractions.
		\item Low coefficient of transformation.
		\item Support by the largest IT giants.
		\item Auto scaling.
	\end{itemize}
	\par\textit{Incremental parallelization.} OpenMP allows you to parallelize a sequen-tial program using small iteration-edits, each of which will increase the para-lelism coefficient of the program. This feature is unique because most other technologies suggest a significant change in the structure of the parallelized program at the first stage of the parallelization process, i.e. The first workable parallel version of the program appears after a long process of debugging and programming of new components, which are inevitably added during parallelization. OpenMP does not have this lack.
	\par\textit{Backward compatibility..} Most software technologies evolve with back-ward compatibility when a newer version of the program supports the functiona-lity of old files. The term \textit{forward compatibility} has the opposite meaning: files created in the new version program remain functional when using the old version of the program. In the case of OpenMP, this means that the parallelized program will be correctly compiled in single-threaded mode even on an old compiler that does not support OpenMP. It is important to note that direct compatibility is ensured if, during parallelization, library functions of OpenMP are not used, and only preprocessor directives are present. If you have library functions, to ensure backward compatibility, you will need to write stub functions in the "omp.h"\ file (some compilers can generate these stubs using a special key).
	\par\textit{High level of abstractions.} A single preprocessor OpenMP directive after processing by the compiler leads to a significant transformation of the source program with the addition of a large number of new logic, which is responsible for determining the number of processors available in the system, for starting and destroying threads, for distributing work between threads, etc. OpenMP takes care of all these operations; in return, the programmer receives a set of very high-level parallelization tools. High-level languages have a traditional drawback: in OpenMP there is no way to change some internal details of working with streams (for example, you cannot set the affinity of streams or reduce the overhead of creating / deleting streams).
	\par\textit{Low parallel transformation ratio (PTR)}. When parallelizing an existing sequential program, we need to make a fairly large number of changes to it. Let PTR be the ratio of the lines of the new program code that was added as a result of parallelization to the total number of lines of code in the program. In OpenMP, the CBT is usually significantly lower than most other parallelization technologies. This is due to the high level of abstraction of the OpenMP language (see the previous paragraph).
	\par\textit{Support by the largest IT giants.} Already during the development of OpenMP, it was supported by the largest players in the IT world. This ensured not only the high quality of standard development, but also the availability of off-the-shelf implementations of the standard in popular compilers. Despite the past twenty years, OpenMP has not lost its adherents and support for the latest versions with a fairly low delay appears in compilers. For example, with the current version of the OpenMP 4.5 standard, the most popular compilers already support OpenMP 4.0. The only exception is Microsoft. Their compiler for several versions invariably only supports OpenMP 2.0.
	\par\textit{Auto scaling.}  
	Low-level parallelization technologies (POSIX Threads, OpenCL) offer the programmer to manually control the number of threads created when performing parallel work. This provides the ability to flexibly manage and configure the process of creating threads depending on the number of processors (cores) available to the system, but it requires a large amount of non-automated work from the programmer. In OpenMP, scaling is controlled automatically, i.e. OpenMP itself asks the number of available processors from the operating system and selects the number of threads to create. But if necessary, OpenMP leaves the ability to set the required number of threads manually.
	\par\textbf{Examples of OpenMP programs.} Consider the simplest examples of running parallel programs below, starting with the traditional programming example ''Hello, World'':
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample1.cpp}
	\end{figure}
	\par The result of the work will be displayed several times in the console message. The number of messages is determined by the number of logical processors available to the system (for example, when using HyperThreading technology with two cores, the number of logical processors will be four). 
	\par The pragma directive extends to the next executable block. In this case, it is a call to the printf function, but one could enclose an arbitrary number of operations in curly braces to expand the executable block:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample2.cpp}
	\end{figure}
	In this program, a block of operations enclosed in braces is executed at the same time on several cores. At the same time, in line 5, the processor is instructed to perform the operation "\texttt{i++}"\ atomically, i.e. not in parallel, but sequentially by each of the threads.
	\par On the one hand, this leads to the fact that the increment operation ceases to be parallelized, which reduces the speed of multi-core execution. On the other hand, the atomic directive is necessary in this case, because otherwise, a difficultly detectable problem with data race could arise, which manifests itself in a conflict when writing data to a common memory area simultaneously by several threads in the variable i. Note that the atomic directive can only be used for single-line simple assignment commands.
	\par To isolate more complex compound commands with the possible invo-cation of user and system functions, you should use the critical directive, which allows (unlike the atomic directive) the ability to expand its scope to a block of operations enclosed in curly braces, with each critical section having the name , allowing you to group different critical sections by this name to prevent the appearance of a single critical section distributed throughout the program:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample3.cpp}
	\end{figure}
	\par In this case, the printf function on line 4 is executed by all threads in parallel, which can lead to mixing of the output characters. On the contrary, the printf function in line 8 is executed by threads strictly in turn, which prevents possible conflicts between them, but slows down program execution due to the artificial limitation of the parallelism coefficient.
	\par An example of parallelizing a program containing a sequential call to the functions \texttt{run\textunderscore function1} and \texttt{ run\textunderscore function2}, which are independent of each other (i.e. do not use common data and the results of one do not affect the results of the other) and therefore allow convenient \textit{instruction parallelization} in pure form:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample4.cpp}
	\end{figure}
	\par Consider an example of loop parallelization using OpenMP. Suppose that in each cell of a one-dimensional array you need to write the index of this cell raised to the sixth power:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample5.cpp}
	\end{figure}
	\par Let the specified program run on a dual-core processor. Then the first processor calculates the values from a [0] to a [4], the second processor calculates the values from a [5] to a [9]. Apparently, when writing to the array, the processor does not interfere with each other, because work with different parts of the array. Let's try to optimize the previous option, reducing the number of multiplication operations for raising to the sixth power:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample6.cpp}
	\end{figure}
	\par In this case, the program will work correctly only if there is one processor (core). If there are several cores, the state of the data race will be observed while writing a new value to the tmp variable (line 4) by several threads, as a result, the array will be filled incorrectly. For example, suppose that the first thread performing iteration i = 2 wrote the number 8 in tmp. Now, when calculating a [2], the thread will try to write the number 8 * 8, however, if the second thread working with the iteration i = 7 has time to wedge before line 5, then the tmp value turns into $7 * 7 * 7$, and the value of a [2] calculated by the first stream turns into $ 7 ^ 6 $, instead of the 64 ones. Lets correct the error as follows:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample7.cpp}
	\end{figure}
	\par A new element has appeared in the directive to the preprocessor: private. This element sets a comma-separated list of local (private) variables for each stream. In this case, there is only one such variable: tmp. Another equivalent way to correct the error is to transfer the declaration of the variable "\texttt{int tmp}"\ inside the parallel region, which forces OpenMP to consider this variable local for each stream. The question may arise, why \texttt{i} was not added to the list of local variables. The answer is not obvious: by default, OpenMP considers the parallelized loop variable local.
	\par Any variable declared inside the parallel region is considered local in OpenMP, so such variables do not need to be specified in the list. Any variable declared outside this area is global (in our case, the global variable is a pointer to the array $a$). But if you need to explicitly indicate the globality of the variable, next to the private command, use the shared ($x$, \ldots) command, where $x$ defines the list of global variables.
	\par Let's consider an example in which you need to calculate the amount and for further execution to form an array of elements of the following series: $\{\;1^i,\;2^i,\;3^i,\;4^i,\;5^i\;\}$ for various values $i$, eg: $i\;=\;1,\;2,\;3$. Below is a solution to the problem, but intentionally make a mistake in it:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample8.cpp}
	\end{figure}
	\par In line 2, the parallel rsection starts, but the programmer forgets to indicate that the variables\textit{ j} and the tmp array must be local for each thread. Indeed, in line 4 there is an increment of the variable j common for the threads, which is executed by all threads simultaneously. In this situation, threads can interfere with each other by overwriting the value of\textit{ j.} We fix both errors as follows:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample9.cpp}
	\end{figure}
	\par We see that now the variable \textit{j} is explicitly defined as local (private). If we use the tmp array, the solution will be different - it can fit inside the parallel section (i.e. each stream will have its own instance of the tmp array independent of the others). Why can'twe just specify the tmp variable in the list of the private command, as was done for \textit{j}? The answer is related to the specifics of the C language: the \textit{tmp} variable is a pointer that does not change during the operation of the loop, but the contents of the memory pointed to by \textit{tmp }change. This means that specifying \textit{tmp} as a private variable would not solve the problem with data racing, because all streams would receive the same tmp address and interfere with each other, writing new values to this address.
	\par Let's consider one more error typical for parallel computing. The following program calculates the sum of numbers from 1 to 100:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample10.cpp}
	\end{figure}
	\par The \textit{sum} variable is global, so when you try to write a new value to it, the threads will interfere with each other. To fix the error, we have to use the local amount for each thread, and then we need to add all these local amounts:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample11.cpp}
	\end{figure}
	We see the beginning of the parallel area on line 2 – at this point OpenMP creates several threads. In line 6, new threads are not created (because the parallel keyword is missing), but the threads entering the loop divide iterations among themselves, and do not complete each iteration as a whole. In line 8, the  thread calculating its partial amount tries to add this amount to the total amount. This has to be done using the atomic directive, which ensures that threads will not interfere with each other when overwriting sum.
	
	\par Another difficult point is the reinitialization of the variable \textit{sum\textunderscore private} on line 4: this is necessary because OpenMP does not initialize local variables, even if there are global variables with identical names. This solution is designed to reduce the overhead of copying variables.
	\par The described approach is working, but it is almost never used in practice, because the OpenMP standard for a whole class of similar tasks offers a higher-level and simpler solution. It consists in using the \textit{reduction} command:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample12.cpp}
	\end{figure}
	\par The \textit{reduction} command marks the listed variables as local, and at the end of the parallel region, all local variables are combined (aggregated) into one global variable with the same name using the specified operation. In our case, the operation is the amount. But OpenMP allows the use of  ''*'', ''-'', ''/'' instead of the sign ''+''. It is important that reduction, among other things, does not initialize the variables with the values of the original global variables, but with the most appropriate values of the aggregation logic: for example, when summing, the variable is initialized to zero, and when multiplying, it is initialized to one.
	\par When parallelizing a cycle, iterations may turn out to be unequal in the amount of work performed among themselves. This can lead to the fact that one thread will cope with the selected part of iterations much faster than the second thread and will be idle. To solve this problem, OpenMP offers four different ways to distribute iterations over threads. 
	\begin{itemize}
		\item\textit{Default method:} iterations are divided by the number of parts equal to the number of threads; each thread then performs its part and cannot take someone else's work.
		\item\textit{Static distribution:} iterations are divided into parts of the size specified by users; then, before starting work, each thread receives a fixed number of parts and performs only them without the ability to switch to others.
		\item\textit{Dynamic distribution:} iterations are divided into parts of the size specified by users; then the cycle starts immediately and each thread receives a new part of iterations as the work on the previous one is completed.
		\item\textit{Guided distribution:} the compiler divides the iterations into the number of parts equal to twice the number of threads; then the cycle starts immediately and each thread receives a new part of iterations as the work on the previous one is completed, while the size of the newly issued part decreases compared to the previous time, but cannot become less than the constant value specified by the user.
	\end{itemize}
	\par The user parameter mentioned in each method is called  \textit{chunk\_size}. Each of these methods has its own field of application in which it can provide maximum parallel acceleration. Note that the dynamic and guided modes, despite their logical nature, also have their drawbacks: they require significant overhead during the cycle compared to static. It is also important to understand that when choosing the number of chunk \ textunderscore size it is necessary to take into account the features of the caching mechanism.
	\par Consider an example of a static iteration distribution:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample13.cpp}
	\end{figure}
	\par With three cores, OpenMP will create three threads. The first thread will get iterations $i=1,\;4,\;7,\;…,\;97$ the second – iterations $i=2,\;5,\;8,\;…,\;98$, the third -  $i=3,\;6,\;9,\;…,\;99$. Note that choosing a small value for the parameter chunk\textunderscore size = 1 in this case does not have any negative effects. However, if i were used as an index when accessing the array, the proposed partitioning would lead to memory access not sequentially by sequential addresses, but sparse with step 3, which would degrade cache hit performance when using caching.
	\par Lets consider another example: 
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample14.cpp}
	\end{figure}
	\par Here is an example of how you can tell OpenMP the number of threads to create using the \textit{num\textunderscore threads} option (line 2), without focusing on the actually available number of cores (processors) on the computer. Next, the three created streams share 100 iterations among themselves in a way that is already familiar to us. However, the nowait option allows the first thread to cope with work not to wait for the others, but to proceed to the next work after the cycle. Behind a cycle in parallel, two functions are performed (lines 9 and 11). Each of the functions is enclosed in a section, which must have the parent element sections. As a result, the first thread freed up after the loop will calculate the function on line 9. The second freed thread will calculate the function on line 11. The third thread will not get work beyond its share of iterations in the first loop.
OpenMP's general requirement for parallelized loops is their \textit {canonicity}. A \texttt{for} loop is called \textit {canonical} if you can calculate the number of upcoming iterations in advance at its start. This is possible if the following conditions are met simultaneously:
	\begin{itemize}
		\item there are no break and return operations inside the loop;
		\item there is no goto operation inside the loop leading outside the loop;
		\item the loop variable (iterator) does not change inside the loop;
	\end{itemize}
In this case, the record of the cycle should have the form "\texttt{for(i=A; i<B; i+=C)}'', where the numbers A, B, C should not change during the operation of the cycle. The second parameter of the loop can use not only the sign ''\texttt{<}'', but also ''\texttt{>}'', ''\texttt{>=}'', ''\texttt{<=}''. The third parameter of the loop can not only increment, but also decrement the loop variable (a short form of the entry  ''\texttt{i++}'' is allowed).
	\par If iteration \textit{k} affects the results of iteration \textit{m}, then the cycle cannot be parallelized, because it is impossible to predict in advance the order of completion of iterations by multiple threads. The responsibility for detecting such conflicts lies with the programmer. For example, OpenMP will not detect the interdependence of iterations and compile the following program:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample15.cpp}
	\end{figure}
	\par In this program thread 0 most likely will not have time to fill in the element \texttt{a[9]} by the time when thread 1 will calculate the value \texttt{a[10] = 2*a[9]}.
	\par
}