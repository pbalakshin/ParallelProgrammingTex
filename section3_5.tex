{ %section3_5
	\subsection{Errors in multithreaded applications}
	\par In addition to the errors common to a programmer, there are a number of errors specific to parallel computing. These errors are caused by the following features of parallel programs:
	\begin{itemize}
		\item\textbf{Thread synchronization.} The programmer must ensure the correct sequence of operations performed by different flows. In the general case, it is impossible to say exactly in what sequence the flow commands will be executed, since the operating system may suspend the flow at any time.
		\item\textbf{Thread interaction.} Also, the programmer should not allow conflicts when accessing memory areas common to threads.
		\item\textbf{Load balancing.} If in a parallelized program one of the threads does 99 \% of the work, then even on a 64-core system parallel acceleration is unlikely to exceed 1.01.
		\item\textbf{Scalability.} In the ideal case, a parallel program should equally well parallel the work performed on any available number of processors. However, this is not easy to achieve and this often leads to hard-to-detect errors.
	\end{itemize}
	\par Further, we consider in more detail the following incomplete list of typical errors that occur in parallel programs regardless of the parallelization technology used:
	\begin{itemize}
		\item Loss of precision in floating point operations;
		\item Deadlocks;
		\item Race conditions;
		\item АВА problems;
		\item Inversion of priorities;
		\item Starvation;
		\item False Sharing.
	\end{itemize}
	\par\textbf{Loss of accuracy} If a parallel program is used for floating-point operations when working with variables located in the threads common memory, then each time the program is launched, a different result of material calculations may be obtained. This is because when several threads work at the same time, it is impossible to accurately predict in what order the operating system will provide the processor with these threads, because at any time, any thread can be temporarily suspended at the discretion of the OS. This in turn leads to an indefinite sequence of floating-point operations, the result of which, as you know, may depend on the order.
	\par Consider an example illustrating the above:
	\begin{figure}[H]
		\lstinputlisting{OpenMPExample16.cpp}
	\end{figure}
	\par The variable s sums up the results of real calculations with eight threads. The result is s = 14.393189. However, if only one thread executes the same program (for this you need to set the parameter num \textunderscore threads to 1 in line 3), the result will be different: $s=14.357357$. The difference between the two values is approximately 0.25 \%.
	\par t turns out that a parallel program can give different results when launched on different platforms. This should be taken into account when verifying parallel programs using their single-threaded unparalleled analogues.
	\par\textbf{Deadlocks.} One of the commonly used synchronization primitives is a mutex, which allows several threads to consistently and sequentially execute critical areas of code located inside parallel sections of code. Critical sections slow down the program, because at each moment of time, only one thread can be inside the critical section. Using mutexes, for example, omp functions are implemented \textunderscore set\textunderscore lock and omp\textunderscore unset\textunderscore lock в OpenMP. When these functions frame a certain section of code, you can make a critical section from it, the entrance to which is controlled by a conditional program lock. In complex programs can use several locks. This can lead to the fact that two threads, capturing several locks, stop the execution of each other without any possibility to exit the state of waiting for each other. This situation is called deadlock.
	\par The simplest example of deadlock is the operation of two threads, the first of which captures lock 1, then lock 2, and the second first captures lock 2, then lock 1. As a result, deadlock occurs if the operations are performed in the following order:
	\begin{itemize}
\item thread1 captured lock 1;
\item thread2 captured lock 2;
\item thread1 endlessly waits for release of lock 2;
\item thread2 endlessly waits for release of lock 1.
	\end{itemize}
	\par One of the unpleasant aspects of the described situation is that far from always mutual blocking occurs when debugging a program, when it could be easily detected and fixed, because the probability of overlapping events in the right way can be very small. As a result, a program that is running and delivered to the customer may ''freeze'' at random times for allegedly unknown reasons. Consider an example of an artificially implemented deadlock in which you can calculate the likelihood of its occurrence upon repeated startup.
	\par In the program below, in line 7, a thread is created that captures castle1, castle2 in an infinite loop and increments the variable s, freeing both locks after that. In line 13, a thread is created, which also infinitely increments s, but captures the locks in a different order: castle2, castle1. Line 19 creates a thread that monitors the state of s, polling this variable every 10 ms. If the last thread detects that the variable s has stopped changing, it prints a message about the deadlock and the program terminates.
	\begin{figure}[H]
		\lstinputlisting{deadlockExample.cpp}
	\end{figure}
	\par The experiments with the above program were carried out on a computer with an Intel Core i5 processor (4 logical processors) with 8 gigabytes of RAM in the Debian Wheezy operating system. The program was run 10,000 times, and 10,000 values of the variable s were obtained at the time the deadlock occurred. The results of these measurements are shown in the Figure~\ref{GraphDeadlock:image} in the form of a histogram of the distribution density s.
	\par In the above figure, the right borders of the columns are marked on the abscissa. The last column contains all hits from 3000 to infinity. The average value of s in the described case turned out to be 2445, i.e. two threads manage to capture and release the locks in an obviously wrong dangerous order without a deadlock approximately 1222.5 times.
	\begin{figure}[H]
		\includegraphics[width=1\linewidth]{GraphDeadlock}
		\caption{\textit{Histogram of the distribution of the number of starts of parallel programs before deadlock occurs}}
		\label{GraphDeadlock:image}
	\end{figure}
	\par To correct the described error, you need to make the lock capture order the same in all threads. It is sometimes advised throughout the program to establish some general rule for locking locks, for example, you can lock locks in alphabetical order.
	\par In addition to the described situation with the wrong order of capture of mutexes, there are other reasons for deadlocks. For example, the reacquisition of the mutex (lock). An incorrectly written program may try to re-capture the lock it has already captured, without first releasing it. In this case, a repeated attempt to capture completely stops the flow. If the program logic requires re-capture of the mutex (for example, to organize recursion), you should use a special subspecies of locks: recursive mutexes.
	\par\textbf{Race conditions} – an error in a parallel program in which the result of the calculation depends on the order in which the code is executed (it may be different each time the parallel program is launched). For example, consider the following situation. One thread changes the value of a global variable. At this time, the second thread prints this value. If the second thread prints the value before the first one changes it, the program will execute correctly, however, if the code is executed later, the new value assigned in the first thread will be displayed.
	\begin{figure}[H]
		\lstinputlisting{raceConditionalExample1.cpp}
	\end{figure}
	\par This problem can happen even in programs in which multi-threaded programming is not used explicitly, but some shared resources are used. For example, if a program copies text from an input field to the clipboard and then text is immediately inserted into another field, then if it runs on a computer alone, it will always work correctly. However, if a program that also uses the clipboard runs at the same time, it can overwrite the value of the clipboard, even if the copy and paste commands are located strictly one after another. Using shared resources, even for a very short time, can cause errors.
	\par Such a manifestation was called the "heisenbug"\ or "floating error"{}. To avoid this situation, it is necessary to block the recording of the new variable value in the first thread until the second thread finishes work. For example, in OpenMP technology, this problem can be solved as follows (save the old value in another variable):
	\begin{figure}[H]
		\lstinputlisting{raceConditionalExample2.cpp}
	\end{figure}
	\par\textbf{ABA problem} is a problem where a thread reading the same value twice "thinks"\ that the data has not changed. For example, the first thread assigned a variable the value \textit{A}. The second thread assigned it the value \textit{B}, and then again \textit{A}. When the first thread reads this variable again, it is equal to A, and it "thinks"\ that nothing has changed. A more practical example from programming: an address is stored in a variable that points to the beginning of the array. The second thread frees memory for the new array with the free function and creates it with the malloc function, which allocated memory in the same place, since this memory area is already free. When the first thread compares the values of the pointer to the array before and after, it sees that they are equal and decides that the array has not changed, although new data is already stored in its place. To solve this problem, you can store a sign that the array has been modified.
	\par\textbf{Priority inversion.} Imagine a situation where there are 3 threads with priorities: high, medium and low, and threads with high and low priority capture the general mutex. Let the low priority thread capture the mutex and begin execution, but the medium priority thread interrupt it. Now, if a thread with a high priority intercepts a priority and starts executing, it will wait for the mutex to be freed, but a thread with a low priority cannot release it, since a thread with a medium priority has replaced it. This problem is solved by assigning all threads the same priority for the duration of the mutex retention.
	\par\textbf{Starvation} occurs when a thread with a low priority is in a ready state for a long time and is idle. Such starvation is caused by a lack of processor time, there is also a starvation caused by the inability to work with data (ban on reading and/or writing). In modern operating systems, this problem is solved as follows: even if the thread has a very low priority, it is still called for execution after a certain amount of time. In your programs, you should intelligently divide tasks between threads so that a thread performing a more important and long task has a higher priority.
	\par\textbf{False sharing} ---  situation that arises with systems that support memory (cache) coherence, in which extra (unnecessary programs at this point) operations are performed to transfer data between streams. \textit{Memory (Caches) Coherence} --- a memory property in which when changing the value of a memory cell by one process, these changes become visible in other processes. Large resources are spent on organizing such memory, since each time the value changes in one thread, the others need to be notified. Consider the following example:
	\begin{figure}[H]
		\lstinputlisting{falseSharingExample.cpp}
	\end{figure}
	\par If you run the functions \texttt{fprint\textunderscore a} and \texttt{fprint \textunderscore b} in two different threads, then due to the constant synchronization of memory between the threads, the program will run slowly, since a and b are on the same cache line (usually 64 bytes). It would be more reasonable to parallelize each loop between threads (for example, using the preprocessor directive  \#pragma omp parallel for in OpenMP).
}