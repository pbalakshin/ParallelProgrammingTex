{ %section6_1
	\subsection{Work sequence}
	\begin{enumerate}
		\item Add the following OpenMP Directive to all for-loops in the program from Laboratory research \#1: \\	
"\texttt{{\small \#pragma omp parallel for default(none) private(…)\\ shared(…)}}"{}. The presence of all the listed parameters in the specified directive is mandatory.
		\item Check all for-loops for internal dependencies on data between iterations. If dependencies are found, use the directive ''{\small \texttt{\#pragma omp critical}}'' or ''{\small \texttt{\#pragma omp atomic}}'' (if the operation is atomic), or the reduction parameter (preferably), or refuse to parallelize the loop at all (your choice must be justified) to protect critical sections.
		\item Make sure that the resulting program has the property of direct compatibility with compilers that do not support OpenMP (you can compile the program without the option ''\texttt{–fopenmp}'' to check this. As a result, there should be no error messages, and the program should work correctly).
		\item Conduct experiments by measuring the parallel acceleration. Compare parallel acceleration graphs with Laboratory research \#1 and \#2.
		\item Conduct the experiments by adding the "\texttt{schedule}"\ parameter and varying the schedule type in the experiments. You need to do the research for all possible schedules: static, dynamic, guided. You can choose the method of varying the \texttt{chunk\textunderscore size} parameter yourself (but there must be at least 5 points of variation). Compare the parallel acceleration for different schedules with the results of article 4.
		\item Choose the best option for different $N$ from those discussed in item 4 and 5. Formulate the conditions under which best results would be obtained by using other types of writing.
		\item Find the computational complexity of the algorithm before and after parallelization and compare the results obtained.
		\item Write a report on the work performed.
		\item Be ready to answer questions on the presentation.
		\item\textbf{Optional task \#1} (to get good and excellent mark). Give a graph of the CPU load (cores) from the time when the program is running at $N=N_1$ for the best parallelization option to illustrate that the program is really parallelized. You can write a script or just make a screenshot of the Task Manager, specifying the start and the end points of the experiment on the screenshot to get a graph (you need to give the text of the script or the name of the used Manager in the report). You need to provide a graph of the load changes over the entire time of the program execution, because providing a single instant load measurement by the htop utility is not enough.
		\item\textbf{Optional task \#2} (to get excellent mark). Build a parallel acceleration graph for points $N<N_1$ and find the values of $N$ at which the overhead of parallelization exceeds the gain from parallelization (independently for different types of schedules).
	\end{enumerate}
}